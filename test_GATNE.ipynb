{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0de83a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random walks for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "15640it [00:05, 2970.74it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random walks for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "43300it [00:01, 35621.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish generating the walks\n",
      "Saving walks for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 15640/15640 [00:00<00:00, 656645.79it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving walks for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 43300/43300 [00:00<00:00, 699196.00it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting vocab for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 15640/15640 [00:00<00:00, 903528.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting vocab for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████| 43300/43300 [00:00<00:00, 818605.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training pairs for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 15640/15640 [00:00<00:00, 87329.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training pairs for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 43300/43300 [00:00<00:00, 86585.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating neighbors for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 1619/1619 [00:00<00:00, 1779501.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating neighbors for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████| 8360/8360 [00:00<00:00, 1871098.26it/s]\n",
      "epoch 0:   0%|| 40/31312 [00:00<02:35, 201.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 4.161587238311768, 'loss': 4.161587238311768}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:  16%|| 5031/31312 [00:23<01:59, 219.17it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 5000, 'avg_loss': 3.173034992343877, 'loss': 2.4089150428771973}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:  32%|| 10031/31312 [00:49<01:49, 194.33it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 10000, 'avg_loss': 2.809113660009846, 'loss': 2.3913214206695557}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:  48%|| 15013/31312 [01:45<02:54, 93.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 15000, 'avg_loss': 2.6470494246007634, 'loss': 2.266148090362549}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:  64%|| 20006/31312 [02:57<01:59, 94.35it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 20000, 'avg_loss': 2.5322273216881244, 'loss': 2.189408540725708}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:  80%|| 25005/31312 [06:01<03:27, 30.37it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 25000, 'avg_loss': 2.4330014767472656, 'loss': 1.9068797826766968}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:  96%|| 30010/31312 [08:26<00:20, 63.43it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 30000, 'avg_loss': 2.342067594595907, 'loss': 1.82365083694458}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0: 100%|| 31312/31312 [09:07<00:00, 57.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Classification] Macro-F1: 0.4141 (0.0180) | Micro-F1: 0.4394 (0.0037)\n",
      "\t[Clustering] NMI: 0.0057 | 0.0002\n",
      "\t[Similarity] [5,10,20,50,100] : [0.4521,0.4276,0.4097,0.3932,0.3842]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:   0%|| 5/31312 [00:00<50:03, 10.42it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 0, 'avg_loss': 1.851701259613037, 'loss': 1.851701259613037}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:  16%|| 5005/31312 [01:37<11:31, 38.03it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 5000, 'avg_loss': 1.7149731583701113, 'loss': 1.5067921876907349}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:  32%|| 10018/31312 [02:39<03:21, 105.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 10000, 'avg_loss': 1.6571510872260629, 'loss': 1.3738837242126465}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:  48%|| 15026/31312 [03:39<01:42, 159.42it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 15000, 'avg_loss': 1.6047319251452674, 'loss': 1.3692924976348877}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:  64%|| 20005/31312 [04:17<01:27, 129.08it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 20000, 'avg_loss': 1.5564500445169507, 'loss': 1.3314712047576904}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:  80%|| 25021/31312 [05:03<01:08, 92.42it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 25000, 'avg_loss': 1.5123617521672577, 'loss': 1.2375247478485107}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1:  96%|| 30027/31312 [06:16<00:09, 140.06it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 1, 'iter': 30000, 'avg_loss': 1.4712961483969655, 'loss': 1.2669026851654053}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 1: 100%|| 31312/31312 [06:25<00:00, 81.12it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Classification] Macro-F1: 0.4251 (0.0110) | Micro-F1: 0.4529 (0.0040)\n",
      "\t[Clustering] NMI: 0.0050 | 0.0005\n",
      "\t[Similarity] [5,10,20,50,100] : [0.4508,0.4227,0.3995,0.3828,0.374]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2:   0%|| 9/31312 [00:00<14:01, 37.19it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 0, 'avg_loss': 1.259683609008789, 'loss': 1.259683609008789}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2:  16%|| 5023/31312 [00:40<03:21, 130.76it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 5000, 'avg_loss': 1.1882793993574217, 'loss': 1.0462530851364136}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2:  32%|| 10030/31312 [01:17<02:07, 167.56it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 10000, 'avg_loss': 1.1615398831932966, 'loss': 0.9207583665847778}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2:  48%|| 15030/31312 [01:53<01:35, 169.87it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 15000, 'avg_loss': 1.1369225733534956, 'loss': 1.0339769124984741}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2:  64%|| 20024/31312 [02:33<01:09, 163.36it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 20000, 'avg_loss': 1.1138905060570823, 'loss': 1.1325461864471436}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2:  80%|| 25015/31312 [03:11<00:51, 123.25it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 25000, 'avg_loss': 1.0934046220123317, 'loss': 0.8924440145492554}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2:  96%|| 30028/31312 [03:56<00:06, 190.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 2, 'iter': 30000, 'avg_loss': 1.0736286996233515, 'loss': 0.8835773468017578}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 2: 100%|| 31312/31312 [04:02<00:00, 129.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Classification] Macro-F1: 0.4530 (0.0058) | Micro-F1: 0.4633 (0.0049)\n",
      "\t[Clustering] NMI: 0.0075 | 0.0007\n",
      "\t[Similarity] [5,10,20,50,100] : [0.454,0.431,0.4089,0.3895,0.3795]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3:   0%|| 20/31312 [00:00<05:21, 97.41it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 0, 'avg_loss': 0.8716562986373901, 'loss': 0.8716562986373901}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3:  16%|| 5040/31312 [00:38<01:56, 225.95it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 5000, 'avg_loss': 0.9337514494567174, 'loss': 0.9951727390289307}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3:  32%|| 10011/31312 [01:07<02:29, 142.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 10000, 'avg_loss': 0.9200489818900839, 'loss': 0.9295198917388916}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3:  48%|| 15018/31312 [01:46<01:41, 161.02it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 15000, 'avg_loss': 0.9069330028419248, 'loss': 0.8537870645523071}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3:  64%|| 20020/31312 [02:26<01:26, 130.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 20000, 'avg_loss': 0.8943198235873776, 'loss': 0.8057821989059448}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3:  80%|| 25012/31312 [03:14<00:59, 105.62it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 25000, 'avg_loss': 0.8825019435149986, 'loss': 0.9892697334289551}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3:  96%|| 30026/31312 [03:55<00:08, 156.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 3, 'iter': 30000, 'avg_loss': 0.8714637545771465, 'loss': 0.7882325053215027}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 3: 100%|| 31312/31312 [04:05<00:00, 127.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Classification] Macro-F1: 0.4279 (0.0110) | Micro-F1: 0.4507 (0.0043)\n",
      "\t[Clustering] NMI: 0.0054 | 0.0002\n",
      "\t[Similarity] [5,10,20,50,100] : [0.4492,0.423,0.4035,0.3865,0.3781]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4:   0%|| 6/31312 [00:00<22:37, 23.07it/s]  "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 0, 'avg_loss': 0.8000925779342651, 'loss': 0.8000925779342651}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4:  16%|| 5014/31312 [00:40<04:11, 104.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 5000, 'avg_loss': 0.7917295569206472, 'loss': 0.7259659767150879}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4:  32%|| 10016/31312 [01:21<02:33, 138.47it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 10000, 'avg_loss': 0.7845868310944079, 'loss': 0.8781912922859192}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4:  48%|| 15027/31312 [02:03<01:33, 173.55it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 15000, 'avg_loss': 0.7766036276586548, 'loss': 0.8053305149078369}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4:  64%|| 20002/31312 [02:47<01:27, 128.57it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 20000, 'avg_loss': 0.7684882514048001, 'loss': 0.8197945952415466}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4:  80%|| 25024/31312 [03:33<00:41, 153.24it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 25000, 'avg_loss': 0.7611321695761358, 'loss': 0.7535653114318848}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4:  96%|| 30024/31312 [04:21<00:10, 120.92it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 4, 'iter': 30000, 'avg_loss': 0.7542750011700033, 'loss': 0.7324659824371338}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 4: 100%|| 31312/31312 [04:31<00:00, 115.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Classification] Macro-F1: 0.4242 (0.0029) | Micro-F1: 0.4449 (0.0024)\n",
      "\t[Clustering] NMI: 0.0075 | 0.0001\n",
      "\t[Similarity] [5,10,20,50,100] : [0.4468,0.4158,0.3972,0.3792,0.3697]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5:   0%|| 10/31312 [00:00<10:29, 49.73it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 0, 'avg_loss': 0.6961605548858643, 'loss': 0.6961605548858643}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5:  16%|| 5022/31312 [00:40<03:01, 145.14it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 5000, 'avg_loss': 0.7058140904551576, 'loss': 0.6508585214614868}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5:  32%|| 10035/31312 [01:25<01:56, 183.22it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 10000, 'avg_loss': 0.6995878808618295, 'loss': 0.5227296352386475}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5:  48%|| 15024/31312 [02:03<01:44, 155.79it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 15000, 'avg_loss': 0.6952548046865731, 'loss': 0.680739164352417}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5:  64%|| 20016/31312 [02:44<01:30, 124.58it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 20000, 'avg_loss': 0.6902375023717624, 'loss': 0.6451594233512878}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5:  80%|| 25020/31312 [03:20<00:40, 156.29it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 25000, 'avg_loss': 0.6858015487176992, 'loss': 0.670167088508606}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5:  96%|| 30022/31312 [04:00<00:12, 103.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 5, 'iter': 30000, 'avg_loss': 0.6817379499667049, 'loss': 0.6106115579605103}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 5: 100%|| 31312/31312 [04:09<00:00, 125.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Classification] Macro-F1: 0.4126 (0.0025) | Micro-F1: 0.4337 (0.0018)\n",
      "\t[Clustering] NMI: 0.0087 | 0.0003\n",
      "\t[Similarity] [5,10,20,50,100] : [0.4555,0.426,0.4065,0.3866,0.3748]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6:   0%|| 15/31312 [00:00<06:29, 80.34it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 0, 'avg_loss': 0.5853804349899292, 'loss': 0.5853804349899292}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6:  16%|| 5004/31312 [00:40<02:38, 166.19it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 5000, 'avg_loss': 0.6505394384542053, 'loss': 0.6364444494247437}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6:  32%|| 10023/31312 [01:14<01:35, 223.51it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 10000, 'avg_loss': 0.6473237150264447, 'loss': 0.6569089293479919}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6:  48%|| 15040/31312 [01:37<01:11, 227.93it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 15000, 'avg_loss': 0.64405349966415, 'loss': 0.800261378288269}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6:  64%|| 20025/31312 [01:58<00:48, 233.63it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 20000, 'avg_loss': 0.6400775616541248, 'loss': 0.7908459901809692}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6:  80%|| 25040/31312 [02:20<00:27, 225.35it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 25000, 'avg_loss': 0.6366297959599522, 'loss': 0.6721832752227783}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6:  96%|| 30042/31312 [02:43<00:05, 226.98it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 6, 'iter': 30000, 'avg_loss': 0.6337860926664383, 'loss': 0.6649888753890991}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 6: 100%|| 31312/31312 [02:48<00:00, 185.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\t[Classification] Macro-F1: 0.4087 (0.0095) | Micro-F1: 0.4252 (0.0030)\n",
      "\t[Clustering] NMI: 0.0074 | 0.0004\n",
      "\t[Similarity] [5,10,20,50,100] : [0.4542,0.4238,0.3996,0.3831,0.3726]\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "torch.manual_seed(0)\n",
    "torch.cuda.manual_seed_all(0)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from numpy import random\n",
    "from torch.nn.parameter import Parameter\n",
    "from OpenAttMultiGL.utils.dataset import dataset\n",
    "from OpenAttMultiGL.utils.process import * \n",
    "from OpenAttMultiGL.layers.hdmi.gcn import GCN\n",
    "from OpenAttMultiGL.model.GATNE.utils import *\n",
    "from OpenAttMultiGL.model.GATNE.evaluate import evaluate\n",
    "#from mGCN_Toolbox.model.GATNE.walk import *\n",
    "#from OpenAttMultiGL.model.X_GOAL.evaluate import *\n",
    "\n",
    "def combine_att(h_list):\n",
    "    att_act1 = nn.Tanh()\n",
    "    att_act2 = nn.Softmax(dim=-1)\n",
    "    h_combine_list = []\n",
    "    for i, h in enumerate(h_list):\n",
    "        h = w_list[i](h)\n",
    "        h = y_list[i](h)\n",
    "        h_combine_list.append(h)\n",
    "    score = torch.cat(h_combine_list, -1)\n",
    "    score = att_act1(score)\n",
    "    score = att_act2(score)\n",
    "    score = torch.unsqueeze(score, -1)\n",
    "    h = torch.stack(h_list, dim=1)\n",
    "    h = score * h\n",
    "    h = torch.sum(h, dim=1)\n",
    "    return h\n",
    "\n",
    "def embed(seq, adj_list, sparse):\n",
    "    global w_list\n",
    "    global y_list\n",
    "    gcn_list = nn.ModuleList([GCN(ft_size, hid_units) for _ in range(n_networks)])\n",
    "    w_list = nn.ModuleList([nn.Linear(hid_units, hid_units, bias=False) for _ in range(n_networks)])\n",
    "    y_list = nn.ModuleList([nn.Linear(hid_units, 1) for _ in range(n_networks)])\n",
    "    h_1_list = []\n",
    "    for i, adj in enumerate(adj_list):\n",
    "        h_1 = torch.squeeze(gcn_list[i](seq, adj, sparse))\n",
    "        h_1_list.append(h_1)\n",
    "    h = combine_att(h_1_list)\n",
    "    return h.detach()\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def get_batches(pairs, neighbors, batch_size):\n",
    "    n_batches = (len(pairs) + (batch_size - 1)) // batch_size\n",
    "\n",
    "    for idx in range(n_batches):\n",
    "        x, y, t, neigh = [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            index = idx * batch_size + i\n",
    "            if index >= len(pairs):\n",
    "                break\n",
    "            x.append(pairs[index][0])\n",
    "            y.append(pairs[index][1])\n",
    "            t.append(pairs[index][2])\n",
    "            neigh.append(neighbors[pairs[index][0]])\n",
    "        yield torch.tensor(x), torch.tensor(y), torch.tensor(t), torch.tensor(neigh)\n",
    "\n",
    "\n",
    "class GATNEModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a, features\n",
    "    ):\n",
    "        super(GATNEModel, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_u_size = embedding_u_size\n",
    "        self.edge_type_count = edge_type_count\n",
    "        self.dim_a = dim_a\n",
    "\n",
    "        self.features = None\n",
    "        if features is not None:\n",
    "            self.features = features\n",
    "            feature_dim = self.features.shape[-1]\n",
    "            self.embed_trans = Parameter(torch.FloatTensor(feature_dim, embedding_size))\n",
    "            self.u_embed_trans = Parameter(torch.FloatTensor(edge_type_count, feature_dim, embedding_u_size))\n",
    "        else:\n",
    "            self.node_embeddings = Parameter(torch.FloatTensor(num_nodes, embedding_size))\n",
    "            self.node_type_embeddings = Parameter(\n",
    "                torch.FloatTensor(num_nodes, edge_type_count, embedding_u_size)\n",
    "            )\n",
    "        self.trans_weights = Parameter(\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, embedding_size)\n",
    "        )\n",
    "        self.trans_weights_s1 = Parameter(\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, dim_a)\n",
    "        )\n",
    "        self.trans_weights_s2 = Parameter(torch.FloatTensor(edge_type_count, dim_a, 1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.features is not None:\n",
    "            self.embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "            self.u_embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        else:\n",
    "            self.node_embeddings.data.uniform_(-1.0, 1.0)\n",
    "            self.node_type_embeddings.data.uniform_(-1.0, 1.0)\n",
    "        self.trans_weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s1.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s2.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "\n",
    "    def forward(self, train_inputs, train_types, node_neigh):\n",
    "        if self.features is None:\n",
    "            node_embed = self.node_embeddings[train_inputs]\n",
    "            node_embed_neighbors = self.node_type_embeddings[node_neigh]\n",
    "        else:\n",
    "            node_embed = torch.mm(self.features[train_inputs], self.embed_trans)\n",
    "            node_embed_neighbors = torch.einsum('bijk,akm->bijam', self.features[node_neigh], self.u_embed_trans)\n",
    "        node_embed_tmp = torch.diagonal(node_embed_neighbors, dim1=1, dim2=3).permute(0, 3, 1, 2)\n",
    "        node_type_embed = torch.sum(node_embed_tmp, dim=2)\n",
    "\n",
    "        trans_w = self.trans_weights[train_types]\n",
    "        trans_w_s1 = self.trans_weights_s1[train_types]\n",
    "        trans_w_s2 = self.trans_weights_s2[train_types]\n",
    "\n",
    "        attention = F.softmax(\n",
    "            torch.matmul(\n",
    "                torch.tanh(torch.matmul(node_type_embed, trans_w_s1)), trans_w_s2\n",
    "            ).squeeze(2),\n",
    "            dim=1,\n",
    "        ).unsqueeze(1)\n",
    "        node_type_embed = torch.matmul(attention, node_type_embed)\n",
    "        node_embed = node_embed + torch.matmul(node_type_embed, trans_w).squeeze(1)\n",
    "\n",
    "        last_node_embed = F.normalize(node_embed, dim=1)\n",
    "\n",
    "        return last_node_embed\n",
    "\n",
    "\n",
    "class NSLoss(nn.Module):\n",
    "    def __init__(self, num_nodes, num_sampled, embedding_size):\n",
    "        super(NSLoss, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_sampled = num_sampled\n",
    "        self.embedding_size = embedding_size\n",
    "        self.weights = Parameter(torch.FloatTensor(num_nodes, embedding_size))\n",
    "        self.sample_weights = F.normalize(\n",
    "            torch.Tensor(\n",
    "                [\n",
    "                    (math.log(k + 2) - math.log(k + 1)) / math.log(num_nodes + 1)\n",
    "                    for k in range(num_nodes)\n",
    "                ]\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "\n",
    "    def forward(self, input, embs, label):\n",
    "        n = input.shape[0]\n",
    "        log_target = torch.log(\n",
    "            torch.sigmoid(torch.sum(torch.mul(embs, self.weights[label]), 1))\n",
    "        )\n",
    "        negs = torch.multinomial(\n",
    "            self.sample_weights, self.num_sampled * n, replacement=True\n",
    "        ).view(n, self.num_sampled)\n",
    "        noise = torch.neg(self.weights[negs])\n",
    "        sum_log_sampled = torch.sum(\n",
    "            torch.log(torch.sigmoid(torch.bmm(noise, embs.unsqueeze(2)))), 1\n",
    "        ).squeeze()\n",
    "\n",
    "        loss = log_target + sum_log_sampled\n",
    "        return -loss.sum() / n\n",
    "\n",
    "\n",
    "def train_model(network_data, feature_dic,dataset):\n",
    "    vocab, index2word, train_pairs = generate(network_data, args.num_walks, args.walk_length, args.schema, file_name, args.window_size, args.num_workers, args.walk_file)\n",
    "\n",
    "    edge_types = list(network_data.keys())\n",
    "\n",
    "    num_nodes = len(index2word)\n",
    "    edge_type_count = len(edge_types)\n",
    "    epochs = args.epoch\n",
    "    batch_size = args.batch_size\n",
    "    embedding_size = args.dimensions\n",
    "    embedding_u_size = args.edge_dim\n",
    "    u_num = edge_type_count\n",
    "    num_sampled = args.negative_samples\n",
    "    dim_a = args.att_dim\n",
    "    att_head = 1\n",
    "    neighbor_samples = args.neighbor_samples\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    neighbors = generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples)\n",
    "\n",
    "    features = None\n",
    "    if feature_dic is not None:\n",
    "        feature_dim = len(list(feature_dic.values())[0])\n",
    "        print('feature dimension: ' + str(feature_dim))\n",
    "        features = np.zeros((num_nodes, feature_dim), dtype=np.float32)\n",
    "        for key, value in feature_dic.items():\n",
    "            if key in vocab:\n",
    "                features[vocab[key].index, :] = np.array(value)\n",
    "        features = torch.FloatTensor(features).to(device)\n",
    "\n",
    "    model = GATNEModel(\n",
    "        num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a, features\n",
    "    )\n",
    "    nsloss = NSLoss(num_nodes, num_sampled, embedding_size)\n",
    "\n",
    "    model.to(device)\n",
    "    nsloss.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{\"params\": model.parameters()}, {\"params\": nsloss.parameters()}], lr=1e-4\n",
    "    )\n",
    "    best_micro = 0\n",
    "    best_macro = 0\n",
    "    best_score = 0\n",
    "    test_score = (0.0, 0.0, 0.0)\n",
    "    patience = 0\n",
    "    sparse = True\n",
    "    labels = torch.FloatTensor(dataset.gcn_labels)\n",
    "    idx_train = torch.LongTensor(dataset.train_id)\n",
    "    idx_val = torch.LongTensor(dataset.valid_id)\n",
    "    idx_test = torch.LongTensor(dataset.test_id)\n",
    "    macro = []\n",
    "    micro = []\n",
    "    k1_list = []\n",
    "    sim_list = []\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_pairs)\n",
    "        batches = get_batches(train_pairs, neighbors, batch_size)\n",
    "\n",
    "        data_iter = tqdm(\n",
    "            batches,\n",
    "            desc=\"epoch %d\" % (epoch),\n",
    "            total=(len(train_pairs) + (batch_size - 1)) // batch_size,\n",
    "            bar_format=\"{l_bar}{r_bar}\",\n",
    "        )\n",
    "        avg_loss = 0.0\n",
    "        for i, data in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            embs = model(data[0].to(device), data[2].to(device), data[3].to(device),)\n",
    "            loss = nsloss(data[0].to(device), embs, data[1].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            #print('embs: ', embs)\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if i % 5000 == 0:\n",
    "                post_fix = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"iter\": i,\n",
    "                    \"avg_loss\": avg_loss / (i + 1),\n",
    "                    \"loss\": loss.item(),\n",
    "                }\n",
    "                data_iter.write(str(post_fix))\n",
    "        \n",
    "        model.eval()\n",
    "        features = torch.FloatTensor(preprocessed_features)\n",
    "        gcn_adj_list = [normalize_adj(adj) for adj in dataset.gcn_adj_list]\n",
    "        adj_list = [sparse_mx_to_torch_sparse_tensor(adj) for adj in gcn_adj_list]\n",
    "        embeds = embed(features, adj_list, sparse)\n",
    "        \n",
    "        macro_f1s, micro_f1s, k1, sim = evaluate(embeds, idx_train, idx_val, idx_test, labels)\n",
    "        f1_macro = np.mean(macro_f1s)\n",
    "        f1_micro = np.mean(micro_f1s)\n",
    "        \n",
    "        macro.append(f1_macro)\n",
    "        micro.append(f1_micro)\n",
    "        k1_list.append(k1)\n",
    "        sim_list.append(sim)\n",
    "    #return average_micro,average_macro,average_sim,average_nmi\n",
    "    return macro,micro,k1_list,sim_list\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    file_name = args.input\n",
    "    t = dataset('imdb')\n",
    "    if args.features is not None:\n",
    "        feature_dic = load_feature_data(args.features)\n",
    "    else:\n",
    "        feature_dic = None\n",
    "        \n",
    "    preprocessed_features = preprocess_features(t.features)\n",
    "    ft_size = preprocessed_features[0].shape[1] \n",
    "    hid_units = 128\n",
    "    n_networks = len(t.adj_list)\n",
    "    \n",
    "    #embeds = embed(features, adj_list, self.args.sparse)\n",
    "    # Write down data in format required for model training\n",
    "    #f = open(\"OpenAttMultiGL/data/GATNE/Amazon/testt.txt\",\"a\")\n",
    "    #d = dict()\n",
    "    #for i in range(len(t.sequence_adj)):\n",
    "        #d[i] = []\n",
    "        #for j in range(len(t.sequence_adj[i])):\n",
    "            #for l in t.test_id:\n",
    "                #if j == l:\n",
    "                    #for k in range(len(t.sequence_adj[i][j])):\n",
    "                        #f.write(str(i))\n",
    "                        #f.write(' ')\n",
    "                        #f.write(str(j))\n",
    "                        #f.write(' ')\n",
    "                        #f.write(str(k))\n",
    "                        #f.write(' ')\n",
    "                        #f.write(str(int(t.sequence_adj[i][j][k])))\n",
    "                        #f.write('\\n')\n",
    "    #f.close()\n",
    "    #training_data_by_type = load_training_data(\"OpenAttMultiGL/data/GATNE/\"+file_name + \"/train.txt\")\n",
    "    valid_true_data_by_edge, valid_false_data_by_edge = load_testing_data(\n",
    "        \"OpenAttMultiGL/data/GATNE/\"+file_name + \"/valid.txt\"\n",
    "    )\n",
    "    testing_true_data_by_edge, testing_false_data_by_edge = load_testing_data(\n",
    "        \"OpenAttMultiGL/data/GATNE/\"+file_name + \"/test.txt\"\n",
    "    )\n",
    "    \n",
    "    #c = t.sequence_adj[0][]\n",
    "    d = dict()\n",
    "    for i in range(len(t.sequence_adj)):\n",
    "        d[i] = []\n",
    "        for j in range(len(t.sequence_adj[i])):\n",
    "            for l in t.train_id:\n",
    "                if j == l:\n",
    "                    for k in range(len(t.sequence_adj[i][j])):\n",
    "                        if t.sequence_adj[i][j][k] == 1:\n",
    "                            e = (str(j),str(k))\n",
    "                            d[i].append(e)\n",
    "    #micro,macro,sim,nmi = train_model(d, feature_dic,t)\n",
    "    micro,macro,nmi,sim = train_model(d, feature_dic,t)\n",
    "    \n",
    "    print(\"Final score: \\n\")\n",
    "    print('Micro: {:.4f} ({:.4f})'.format(np.mean(micro),np.std(micro)))\n",
    "    print('Macro: {:.4f} ({:.4f})'.format(np.mean(macro),np.std(macro)))\n",
    "    print('Sim: {:.4f} ({:.4f})'.format(np.mean(sim),np.std(sim)))\n",
    "    print('NMI: {:.4f} ({:.4f})'.format(np.mean(nmi),np.std(nmi)))\n",
    "    #print('SIM: ', sim)\n",
    "    #print('NMI: ', nmi)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d68768f7",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
