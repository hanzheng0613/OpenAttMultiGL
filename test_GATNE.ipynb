{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0de83a7",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Namespace(input='Amazon', features=None, walk_file=None, epoch=100, batch_size=64, eval_type='all', schema=None, dimensions=200, edge_dim=10, att_dim=20, walk_length=10, num_walks=20, window_size=5, negative_samples=5, neighbor_samples=10, patience=5, num_workers=16, fff='/Users/hanzhengwang/Library/Jupyter/runtime/kernel-233e4050-97bf-442a-9ab4-98921e311903.json')\n",
      "Total training nodes: 10099\n",
      "Generating random walks for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "163600it [00:03, 43498.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating random walks for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100100it [00:02, 42049.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish generating the walks\n",
      "Saving walks for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 163600/163600 [00:00<00:00, 757944.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving walks for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 100100/100100 [00:00<00:00, 726758.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting vocab for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 163600/163600 [00:00<00:00, 829505.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Counting vocab for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████| 100100/100100 [00:00<00:00, 751011.24it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training pairs for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 163600/163600 [00:01<00:00, 84932.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating training pairs for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 100100/100100 [00:01<00:00, 82992.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating neighbors for layer 0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 65506/65506 [00:00<00:00, 1718586.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generating neighbors for layer 1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████| 61029/61029 [00:00<00:00, 1902163.77it/s]\n",
      "epoch 0:   0%|| 15/140091 [00:00<30:17, 77.07it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'epoch': 0, 'iter': 0, 'avg_loss': 4.177557468414307, 'loss': 4.177557468414307}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "epoch 0:   3%|| 4787/140091 [00:51<24:14, 93.05it/s]\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 346\u001b[0m\n\u001b[1;32m    337\u001b[0m t \u001b[38;5;241m=\u001b[39m dataset(file_name)\n\u001b[1;32m    338\u001b[0m \u001b[38;5;66;03m#training_data_by_type = load_training_data(\"mGCN_Toolbox/data/GATNE/\"+file_name + \"/train.txt\")\u001b[39;00m\n\u001b[1;32m    339\u001b[0m \u001b[38;5;66;03m#valid_true_data_by_edge, valid_false_data_by_edge = load_valid_data(\u001b[39;00m\n\u001b[1;32m    340\u001b[0m     \u001b[38;5;66;03m#\"mGCN_Toolbox/data/GATNE/\" +file_name + \"/valid.txt\"\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    343\u001b[0m     \u001b[38;5;66;03m#\"mGCN_Toolbox/data/GATNE/\"+ file_name + \"/test.txt\"\u001b[39;00m\n\u001b[1;32m    344\u001b[0m \u001b[38;5;66;03m#)\u001b[39;00m\n\u001b[0;32m--> 346\u001b[0m average_auc, average_f1, average_pr \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_model\u001b[49m\u001b[43m(\u001b[49m\u001b[43mt\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_data_by_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeature_dic\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    348\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall ROC-AUC:\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_auc)\n\u001b[1;32m    349\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOverall PR-AUC\u001b[39m\u001b[38;5;124m\"\u001b[39m, average_pr)\n",
      "Cell \u001b[0;32mIn[1], line 258\u001b[0m, in \u001b[0;36mtrain_model\u001b[0;34m(network_data, feature_dic)\u001b[0m\n\u001b[1;32m    256\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, data \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(data_iter):\n\u001b[1;32m    257\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m--> 258\u001b[0m     embs \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m3\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mto\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdevice\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    259\u001b[0m     loss \u001b[38;5;241m=\u001b[39m nsloss(data[\u001b[38;5;241m0\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device), embs, data[\u001b[38;5;241m1\u001b[39m]\u001b[38;5;241m.\u001b[39mto(device))\n\u001b[1;32m    260\u001b[0m     loss\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m~/anaconda3/lib/python3.10/site-packages/torch/nn/modules/module.py:1130\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *input, **kwargs)\u001b[0m\n\u001b[1;32m   1126\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1127\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1128\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1129\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1130\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1131\u001b[0m \u001b[38;5;66;03m# Do not call functions when jit is used\u001b[39;00m\n\u001b[1;32m   1132\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[38;5;241m=\u001b[39m [], []\n",
      "Cell \u001b[0;32mIn[1], line 148\u001b[0m, in \u001b[0;36mGATNEModel.forward\u001b[0;34m(self, train_inputs, train_types, node_neigh)\u001b[0m\n\u001b[1;32m    142\u001b[0m trans_w_s1 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrans_weights_s1[train_types]\n\u001b[1;32m    143\u001b[0m trans_w_s2 \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrans_weights_s2[train_types]\n\u001b[1;32m    145\u001b[0m attention \u001b[38;5;241m=\u001b[39m F\u001b[38;5;241m.\u001b[39msoftmax(\n\u001b[1;32m    146\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    147\u001b[0m \u001b[43m        \u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtanh\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmatmul\u001b[49m\u001b[43m(\u001b[49m\u001b[43mnode_type_embed\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans_w_s1\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrans_w_s2\u001b[49m\n\u001b[0;32m--> 148\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msqueeze\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m2\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    149\u001b[0m     dim\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m    150\u001b[0m )\u001b[38;5;241m.\u001b[39munsqueeze(\u001b[38;5;241m1\u001b[39m)\n\u001b[1;32m    151\u001b[0m node_type_embed \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(attention, node_type_embed)\n\u001b[1;32m    152\u001b[0m node_embed \u001b[38;5;241m=\u001b[39m node_embed \u001b[38;5;241m+\u001b[39m torch\u001b[38;5;241m.\u001b[39mmatmul(node_type_embed, trans_w)\u001b[38;5;241m.\u001b[39msqueeze(\u001b[38;5;241m1\u001b[39m)\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import math\n",
    "import argparse\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from numpy import random\n",
    "from torch.nn.parameter import Parameter\n",
    "from mGCN_Toolbox.utils.dataset import dataset\n",
    "from mGCN_Toolbox.utils.process import * \n",
    "from mGCN_Toolbox.model.GATNE.utils import *\n",
    "#from mGCN_Toolbox.model.GATNE.walk import *\n",
    "def parse_args():\n",
    "    parser = argparse.ArgumentParser()\n",
    "\n",
    "    parser.add_argument('--input', type=str, default='Amazon',\n",
    "                        help='Input dataset path')\n",
    "    \n",
    "    parser.add_argument('--features', type=str, default=None,\n",
    "                        help='Input node features')\n",
    "\n",
    "    parser.add_argument('--walk-file', type=str, default=None,\n",
    "                        help='Input random walks')\n",
    "\n",
    "    parser.add_argument('--epoch', type=int, default=100,\n",
    "                        help='Number of epoch. Default is 100.')\n",
    "\n",
    "    parser.add_argument('--batch-size', type=int, default=64,\n",
    "                        help='Number of batch_size. Default is 64.')\n",
    "\n",
    "    parser.add_argument('--eval-type', type=str, default='all',\n",
    "                        help='The edge type(s) for evaluation.')\n",
    "    \n",
    "    parser.add_argument('--schema', type=str, default=None,\n",
    "                        help='The metapath schema (e.g., U-I-U,I-U-I).')\n",
    "\n",
    "    parser.add_argument('--dimensions', type=int, default=200,\n",
    "                        help='Number of dimensions. Default is 200.')\n",
    "\n",
    "    parser.add_argument('--edge-dim', type=int, default=10,\n",
    "                        help='Number of edge embedding dimensions. Default is 10.')\n",
    "    \n",
    "    parser.add_argument('--att-dim', type=int, default=20,\n",
    "                        help='Number of attention dimensions. Default is 20.')\n",
    "\n",
    "    parser.add_argument('--walk-length', type=int, default=10,\n",
    "                        help='Length of walk per source. Default is 10.')\n",
    "\n",
    "    parser.add_argument('--num-walks', type=int, default=20,\n",
    "                        help='Number of walks per source. Default is 20.')\n",
    "\n",
    "    parser.add_argument('--window-size', type=int, default=5,\n",
    "                        help='Context size for optimization. Default is 5.')\n",
    "    \n",
    "    parser.add_argument('--negative-samples', type=int, default=5,\n",
    "                        help='Negative samples for optimization. Default is 5.')\n",
    "    \n",
    "    parser.add_argument('--neighbor-samples', type=int, default=10,\n",
    "                        help='Neighbor samples for aggregation. Default is 10.')\n",
    "\n",
    "    parser.add_argument('--patience', type=int, default=5,\n",
    "                        help='Early stopping patience. Default is 5.')\n",
    "    \n",
    "    parser.add_argument('--num-workers', type=int, default=16,\n",
    "                        help='Number of workers for generating random walks. Default is 16.')\n",
    "    \n",
    "    parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "    return parser.parse_args()\n",
    "\n",
    "def get_batches(pairs, neighbors, batch_size):\n",
    "    n_batches = (len(pairs) + (batch_size - 1)) // batch_size\n",
    "\n",
    "    for idx in range(n_batches):\n",
    "        x, y, t, neigh = [], [], [], []\n",
    "        for i in range(batch_size):\n",
    "            index = idx * batch_size + i\n",
    "            if index >= len(pairs):\n",
    "                break\n",
    "            x.append(pairs[index][0])\n",
    "            y.append(pairs[index][1])\n",
    "            t.append(pairs[index][2])\n",
    "            neigh.append(neighbors[pairs[index][0]])\n",
    "        yield torch.tensor(x), torch.tensor(y), torch.tensor(t), torch.tensor(neigh)\n",
    "\n",
    "\n",
    "class GATNEModel(nn.Module):\n",
    "    def __init__(\n",
    "        self, num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a, features\n",
    "    ):\n",
    "        super(GATNEModel, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.embedding_size = embedding_size\n",
    "        self.embedding_u_size = embedding_u_size\n",
    "        self.edge_type_count = edge_type_count\n",
    "        self.dim_a = dim_a\n",
    "\n",
    "        self.features = None\n",
    "        if features is not None:\n",
    "            self.features = features\n",
    "            feature_dim = self.features.shape[-1]\n",
    "            self.embed_trans = Parameter(torch.FloatTensor(feature_dim, embedding_size))\n",
    "            self.u_embed_trans = Parameter(torch.FloatTensor(edge_type_count, feature_dim, embedding_u_size))\n",
    "        else:\n",
    "            self.node_embeddings = Parameter(torch.FloatTensor(num_nodes, embedding_size))\n",
    "            self.node_type_embeddings = Parameter(\n",
    "                torch.FloatTensor(num_nodes, edge_type_count, embedding_u_size)\n",
    "            )\n",
    "        self.trans_weights = Parameter(\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, embedding_size)\n",
    "        )\n",
    "        self.trans_weights_s1 = Parameter(\n",
    "            torch.FloatTensor(edge_type_count, embedding_u_size, dim_a)\n",
    "        )\n",
    "        self.trans_weights_s2 = Parameter(torch.FloatTensor(edge_type_count, dim_a, 1))\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.features is not None:\n",
    "            self.embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "            self.u_embed_trans.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        else:\n",
    "            self.node_embeddings.data.uniform_(-1.0, 1.0)\n",
    "            self.node_type_embeddings.data.uniform_(-1.0, 1.0)\n",
    "        self.trans_weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s1.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "        self.trans_weights_s2.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "\n",
    "    def forward(self, train_inputs, train_types, node_neigh):\n",
    "        if self.features is None:\n",
    "            node_embed = self.node_embeddings[train_inputs]\n",
    "            node_embed_neighbors = self.node_type_embeddings[node_neigh]\n",
    "        else:\n",
    "            node_embed = torch.mm(self.features[train_inputs], self.embed_trans)\n",
    "            node_embed_neighbors = torch.einsum('bijk,akm->bijam', self.features[node_neigh], self.u_embed_trans)\n",
    "        node_embed_tmp = torch.diagonal(node_embed_neighbors, dim1=1, dim2=3).permute(0, 3, 1, 2)\n",
    "        node_type_embed = torch.sum(node_embed_tmp, dim=2)\n",
    "\n",
    "        trans_w = self.trans_weights[train_types]\n",
    "        trans_w_s1 = self.trans_weights_s1[train_types]\n",
    "        trans_w_s2 = self.trans_weights_s2[train_types]\n",
    "\n",
    "        attention = F.softmax(\n",
    "            torch.matmul(\n",
    "                torch.tanh(torch.matmul(node_type_embed, trans_w_s1)), trans_w_s2\n",
    "            ).squeeze(2),\n",
    "            dim=1,\n",
    "        ).unsqueeze(1)\n",
    "        node_type_embed = torch.matmul(attention, node_type_embed)\n",
    "        node_embed = node_embed + torch.matmul(node_type_embed, trans_w).squeeze(1)\n",
    "\n",
    "        last_node_embed = F.normalize(node_embed, dim=1)\n",
    "\n",
    "        return last_node_embed\n",
    "\n",
    "\n",
    "class NSLoss(nn.Module):\n",
    "    def __init__(self, num_nodes, num_sampled, embedding_size):\n",
    "        super(NSLoss, self).__init__()\n",
    "        self.num_nodes = num_nodes\n",
    "        self.num_sampled = num_sampled\n",
    "        self.embedding_size = embedding_size\n",
    "        self.weights = Parameter(torch.FloatTensor(num_nodes, embedding_size))\n",
    "        self.sample_weights = F.normalize(\n",
    "            torch.Tensor(\n",
    "                [\n",
    "                    (math.log(k + 2) - math.log(k + 1)) / math.log(num_nodes + 1)\n",
    "                    for k in range(num_nodes)\n",
    "                ]\n",
    "            ),\n",
    "            dim=0,\n",
    "        )\n",
    "\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        self.weights.data.normal_(std=1.0 / math.sqrt(self.embedding_size))\n",
    "\n",
    "    def forward(self, input, embs, label):\n",
    "        n = input.shape[0]\n",
    "        log_target = torch.log(\n",
    "            torch.sigmoid(torch.sum(torch.mul(embs, self.weights[label]), 1))\n",
    "        )\n",
    "        negs = torch.multinomial(\n",
    "            self.sample_weights, self.num_sampled * n, replacement=True\n",
    "        ).view(n, self.num_sampled)\n",
    "        noise = torch.neg(self.weights[negs])\n",
    "        sum_log_sampled = torch.sum(\n",
    "            torch.log(torch.sigmoid(torch.bmm(noise, embs.unsqueeze(2)))), 1\n",
    "        ).squeeze()\n",
    "\n",
    "        loss = log_target + sum_log_sampled\n",
    "        return -loss.sum() / n\n",
    "\n",
    "\n",
    "def train_model(network_data, feature_dic):\n",
    "    vocab, index2word, train_pairs = generate(network_data, args.num_walks, args.walk_length, args.schema, file_name, args.window_size, args.num_workers, args.walk_file)\n",
    "\n",
    "    edge_types = list(network_data.keys())\n",
    "\n",
    "    num_nodes = len(index2word)\n",
    "    edge_type_count = len(edge_types)\n",
    "    epochs = args.epoch\n",
    "    batch_size = args.batch_size\n",
    "    embedding_size = args.dimensions\n",
    "    embedding_u_size = args.edge_dim\n",
    "    u_num = edge_type_count\n",
    "    num_sampled = args.negative_samples\n",
    "    dim_a = args.att_dim\n",
    "    att_head = 1\n",
    "    neighbor_samples = args.neighbor_samples\n",
    "\n",
    "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "    neighbors = generate_neighbors(network_data, vocab, num_nodes, edge_types, neighbor_samples)\n",
    "\n",
    "    features = None\n",
    "    if feature_dic is not None:\n",
    "        feature_dim = len(list(feature_dic.values())[0])\n",
    "        print('feature dimension: ' + str(feature_dim))\n",
    "        features = np.zeros((num_nodes, feature_dim), dtype=np.float32)\n",
    "        for key, value in feature_dic.items():\n",
    "            if key in vocab:\n",
    "                features[vocab[key].index, :] = np.array(value)\n",
    "        features = torch.FloatTensor(features).to(device)\n",
    "\n",
    "    model = GATNEModel(\n",
    "        num_nodes, embedding_size, embedding_u_size, edge_type_count, dim_a, features\n",
    "    )\n",
    "    nsloss = NSLoss(num_nodes, num_sampled, embedding_size)\n",
    "\n",
    "    model.to(device)\n",
    "    nsloss.to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(\n",
    "        [{\"params\": model.parameters()}, {\"params\": nsloss.parameters()}], lr=1e-4\n",
    "    )\n",
    "\n",
    "    best_score = 0\n",
    "    test_score = (0.0, 0.0, 0.0)\n",
    "    patience = 0\n",
    "    for epoch in range(epochs):\n",
    "        random.shuffle(train_pairs)\n",
    "        batches = get_batches(train_pairs, neighbors, batch_size)\n",
    "\n",
    "        data_iter = tqdm(\n",
    "            batches,\n",
    "            desc=\"epoch %d\" % (epoch),\n",
    "            total=(len(train_pairs) + (batch_size - 1)) // batch_size,\n",
    "            bar_format=\"{l_bar}{r_bar}\",\n",
    "        )\n",
    "        avg_loss = 0.0\n",
    "\n",
    "        for i, data in enumerate(data_iter):\n",
    "            optimizer.zero_grad()\n",
    "            embs = model(data[0].to(device), data[2].to(device), data[3].to(device),)\n",
    "            loss = nsloss(data[0].to(device), embs, data[1].to(device))\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            avg_loss += loss.item()\n",
    "\n",
    "            if i % 5000 == 0:\n",
    "                post_fix = {\n",
    "                    \"epoch\": epoch,\n",
    "                    \"iter\": i,\n",
    "                    \"avg_loss\": avg_loss / (i + 1),\n",
    "                    \"loss\": loss.item(),\n",
    "                }\n",
    "                data_iter.write(str(post_fix))\n",
    "\n",
    "        final_model = dict(zip(edge_types, [dict() for _ in range(edge_type_count)]))\n",
    "        for i in range(num_nodes):\n",
    "            train_inputs = torch.tensor([i for _ in range(edge_type_count)]).to(device)\n",
    "            train_types = torch.tensor(list(range(edge_type_count))).to(device)\n",
    "            node_neigh = torch.tensor(\n",
    "                [neighbors[i] for _ in range(edge_type_count)]\n",
    "            ).to(device)\n",
    "            node_emb = model(train_inputs, train_types, node_neigh)\n",
    "            for j in range(edge_type_count):\n",
    "                final_model[edge_types[j]][index2word[i]] = (\n",
    "                    node_emb[j].cpu().detach().numpy()\n",
    "                )\n",
    "\n",
    "        valid_aucs, valid_f1s, valid_prs = [], [], []\n",
    "        test_aucs, test_f1s, test_prs = [], [], []\n",
    "        for i in range(edge_type_count):\n",
    "            if args.eval_type == \"all\" or edge_types[i] in args.eval_type.split(\",\"):\n",
    "                tmp_auc, tmp_f1, tmp_pr = evaluate(\n",
    "                    final_model[edge_types[i]],\n",
    "                    valid_true_data_by_edge[edge_types[i]],\n",
    "                    valid_false_data_by_edge[edge_types[i]],\n",
    "                )\n",
    "                valid_aucs.append(tmp_auc)\n",
    "                valid_f1s.append(tmp_f1)\n",
    "                valid_prs.append(tmp_pr)\n",
    "\n",
    "                tmp_auc, tmp_f1, tmp_pr = evaluate(\n",
    "                    final_model[edge_types[i]],\n",
    "                    testing_true_data_by_edge[edge_types[i]],\n",
    "                    testing_false_data_by_edge[edge_types[i]],\n",
    "                )\n",
    "                test_aucs.append(tmp_auc)\n",
    "                test_f1s.append(tmp_f1)\n",
    "                test_prs.append(tmp_pr)\n",
    "        print(\"valid auc:\", np.mean(valid_aucs))\n",
    "        print(\"valid pr:\", np.mean(valid_prs))\n",
    "        print(\"valid f1:\", np.mean(valid_f1s))\n",
    "\n",
    "        average_auc = np.mean(test_aucs)\n",
    "        average_f1 = np.mean(test_f1s)\n",
    "        average_pr = np.mean(test_prs)\n",
    "\n",
    "        cur_score = np.mean(valid_aucs)\n",
    "        if cur_score > best_score:\n",
    "            best_score = cur_score\n",
    "            test_score = (average_auc, average_f1, average_pr)\n",
    "            patience = 0\n",
    "        else:\n",
    "            patience += 1\n",
    "            if patience > args.patience:\n",
    "                print(\"Early Stopping\")\n",
    "                break\n",
    "    return test_score\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = parse_args()\n",
    "    file_name = args.input\n",
    "    print(args)\n",
    "    if args.features is not None:\n",
    "        feature_dic = load_txt_feature_data(args.features)\n",
    "    else:\n",
    "        feature_dic = None\n",
    "    t = dataset(file_name)\n",
    "    #training_data_by_type = load_training_data(\"mGCN_Toolbox/data/GATNE/\"+file_name + \"/train.txt\")\n",
    "    #valid_true_data_by_edge, valid_false_data_by_edge = load_valid_data(\n",
    "        #\"mGCN_Toolbox/data/GATNE/\" +file_name + \"/valid.txt\"\n",
    "    #)\n",
    "    #testing_true_data_by_edge, testing_false_data_by_edge = load_testing_data(\n",
    "        #\"mGCN_Toolbox/data/GATNE/\"+ file_name + \"/test.txt\"\n",
    "    #)\n",
    "\n",
    "    average_auc, average_f1, average_pr = train_model(t.training_data_by_type, feature_dic)\n",
    "\n",
    "    print(\"Overall ROC-AUC:\", average_auc)\n",
    "    print(\"Overall PR-AUC\", average_pr)\n",
    "    print(\"Overall F1:\", average_f1)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "884699a5",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
