{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe9e1abc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Finish loading data\n",
      "Epoch: 0\n",
      "Macro_F1: 0.06469402277039848\n",
      "Micro_F1: 0.14861735458384417\n",
      "Epoch: 1\n",
      "Macro_F1: 0.12094608551952075\n",
      "Micro_F1: 0.3190301048903419\n",
      "Epoch: 27\n",
      "Macro_F1: 0.2652049192632302\n",
      "Micro_F1: 0.32420651137447215\n",
      "Epoch: 32\n",
      "Macro_F1: 0.3028949984540449\n",
      "Micro_F1: 0.3274758207328702\n",
      "Epoch: 38\n",
      "Macro_F1: 0.32459267658482904\n",
      "Micro_F1: 0.35063342868818964\n",
      "Epoch: 43\n",
      "Macro_F1: 0.4046747065746202\n",
      "Micro_F1: 0.3879580438632339\n",
      "Epoch: 44\n",
      "Macro_F1: 0.41159739790493366\n",
      "Micro_F1: 0.3951777686963629\n",
      "Epoch: 48\n",
      "Macro_F1: 0.4121973997525029\n",
      "Micro_F1: 0.4094809971393543\n",
      "Epoch: 50\n",
      "Macro_F1: 0.41411082778592084\n",
      "Micro_F1: 0.424329110475412\n",
      "Epoch: 57\n",
      "Macro_F1: 0.49112614945862126\n",
      "Micro_F1: 0.4737774145211824\n"
     ]
    }
   ],
   "source": [
    "# Test file for toolbox\n",
    "\n",
    "import argparse\n",
    "from OpenAttMultiGL.model.mGCN.mGCN_node import*\n",
    "from OpenAttMultiGL.utils.process import *\n",
    "from OpenAttMultiGL.layers.hdmi.gcn import GCN\n",
    "from OpenAttMultiGL.model.mGCN.evaluate import evaluate\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch\n",
    "from OpenAttMultiGL.utils.dataset import dataset\n",
    "from OpenAttMultiGL.utils.process import split_node_data\n",
    "from sklearn.metrics import roc_auc_score\n",
    "import numpy as np\n",
    "import random\n",
    "import copy\n",
    "import torch.nn.functional as F\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score, recall_score, f1_score\n",
    "from sklearn.metrics import normalized_mutual_info_score, pairwise, f1_score\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "parser = argparse.ArgumentParser(description='GCN')\n",
    "parser.add_argument('--dataset', type=str, default='amazon')\n",
    "parser.add_argument('--fast_split', action='store_true',\n",
    "                    help=\"for large custom datasets (not OGB), do a fast data split\")\n",
    "\n",
    "parser.add_argument('--runs', type=int, default=1)\n",
    "parser.add_argument('--hidden', type=int, default=128,\n",
    "                    help='Number of hidden units.')\n",
    "parser.add_argument('--epochs', type=int, default=500,\n",
    "                    help='Number of training epochs.')\n",
    "parser.add_argument('--alpha', type=float, default=0.6,\n",
    "                    help='Hyperparameter')\n",
    "parser.add_argument('--dropout', type=float, default=0.5,\n",
    "                    help='Dropout')\n",
    "parser.add_argument('--training_ratio', type=float, default=0.3,\n",
    "                    help='Training Ratio')\n",
    "parser.add_argument('--lr', type=float, default=0.001,\n",
    "                    help='Learning Rate')\n",
    "parser.add_argument('--weight_decay', type=float, default=1e-2,\n",
    "                    help='Weight_decay')\n",
    "parser.add_argument('--test_view', type=int, default=1,\n",
    "                    help='Number of training epochs.')\n",
    "parser.add_argument(\"-f\", \"--fff\", help=\"a dummy argument to fool ipython\", default=\"1\")\n",
    "\n",
    "args = parser.parse_args()\n",
    "\n",
    "def evaluate_metrics(true, pred):\n",
    "    preds = pred.max(1)[1].type_as(labels)\n",
    "    correct = preds.eq(true).double()\n",
    "    correct = correct.sum()\n",
    "    return correct /len(true)\n",
    "\n",
    "def evaluate_model(ind):\n",
    "    model_mGCN.eval()\n",
    "    \n",
    "    logits = model_mGCN(sample_data.dataset)\n",
    "    nb_classes = sample_data.num_classes\n",
    "    \n",
    "    pred = logits.max(1).indices\n",
    "    macro_f1 = f1_score(sample_data.labels[ind].cpu().numpy(), logits.max(1)[1][ind].detach().cpu().numpy(),\n",
    "                       average=\"macro\")\n",
    "    micro_f1 = f1_score(sample_data.labels[ind].cpu().numpy(), logits.max(1)[1][ind].detach().cpu().numpy(),\n",
    "                       average=\"micro\")\n",
    "    #t = torch.LongTensor(sample_data.gcn_labels[sample_data.test_id]) \n",
    "    #test_lbls = torch.argmax(t, dim=1)\n",
    "    #nmi = run_kmeans(sample_data.labels,pred, nb_classes)\n",
    "    \n",
    "    return macro_f1,micro_f1\n",
    "    # return evaluate_metrics(labels[ind], logits[ind]).item()\n",
    "soft = nn.Softmax(dim=1)\n",
    "\n",
    "def run_kmeans(x, y, k):\n",
    "    estimator = KMeans(n_clusters=k,n_init=10)\n",
    "\n",
    "    NMI_list = []\n",
    "    for i in range(10):\n",
    "        estimator.fit(x)\n",
    "        y_pred = estimator.predict(x)\n",
    "        s = normalized_mutual_info_score(y, y_pred, average_method='arithmetic')\n",
    "        NMI_list.append(s)\n",
    "\n",
    "    mean = np.mean(NMI_list)\n",
    "    std = np.std(NMI_list)\n",
    "    #print('\\t[Clustering] NMI: {:.4f} | {:.4f}'.format(mean, std))\n",
    "    return mean\n",
    "\n",
    "def write_results(ind):\n",
    "    model_mGCN.eval()\n",
    "    logits = soft(model_mGCN(sample_data.dataset))\n",
    "    f= open(\"./results/\" + args.dataset + \"_combined\", \"w\")\n",
    "    for temp in logits[ind].detach().cpu().numpy():\n",
    "        f.write(\" \".join(np.array([str(i) for i in temp])) +\"\\n\")\n",
    "    f.close()\n",
    "    \n",
    "def combine_att(h_list):\n",
    "    att_act1 = nn.Tanh()\n",
    "    att_act2 = nn.Softmax(dim=-1)\n",
    "    h_combine_list = []\n",
    "    for i, h in enumerate(h_list):\n",
    "        h = w_list[i](h)\n",
    "        h = y_list[i](h)\n",
    "        h_combine_list.append(h)\n",
    "    score = torch.cat(h_combine_list, -1)\n",
    "    score = att_act1(score)\n",
    "    score = att_act2(score)\n",
    "    score = torch.unsqueeze(score, -1)\n",
    "    h = torch.stack(h_list, dim=1)\n",
    "    h = score * h\n",
    "    h = torch.sum(h, dim=1)\n",
    "    return h\n",
    "\n",
    "def embed(seq, adj_list, sparse):\n",
    "    global w_list\n",
    "    global y_list\n",
    "    gcn_list = nn.ModuleList([GCN(ft_size, hid_units) for _ in range(n_networks)])\n",
    "    w_list = nn.ModuleList([nn.Linear(hid_units, hid_units, bias=False) for _ in range(n_networks)])\n",
    "    y_list = nn.ModuleList([nn.Linear(hid_units, 1) for _ in range(n_networks)])\n",
    "    h_1_list = []\n",
    "    for i, adj in enumerate(adj_list):\n",
    "        h_1 = torch.squeeze(gcn_list[i](seq, adj, sparse))\n",
    "        h_1_list.append(h_1)\n",
    "    h = combine_att(h_1_list)\n",
    "    return h.detach()\n",
    "\n",
    "    \n",
    "def run_similarity_search(true_label,pred_label):\n",
    "\n",
    "    c = 0\n",
    "        \n",
    "    for i in range(len(true_label)):\n",
    "        if pred_label[i] == true_label[i]:\n",
    "            c += 1\n",
    "        \n",
    "    sim = c/len(true_label)\n",
    "    return sim\n",
    "\n",
    "best_val = 0\n",
    "best_test = 0\n",
    "\n",
    "def train_model(epochs):\n",
    "    global best_val\n",
    "    global best_test\n",
    "    global best_hits\n",
    "    best_val = 0\n",
    "    best_test = 0\n",
    "    best_macro = 0\n",
    "    best_micro = 0\n",
    "    # print(split_edges['train']['edge'].shape[0])\n",
    "    # training_negative = split_edges['train']['edge_neg'][range(0, split_edges['train']['edge'].shape[0])]\n",
    "    training_negative = []\n",
    "    training_positive = []\n",
    "    sparse = True\n",
    "    macro_list = []\n",
    "    micro_list = []\n",
    "    k1_list = []\n",
    "    sim_list = []\n",
    "    nmi_list = []\n",
    "    labels = torch.FloatTensor(sample_data.gcn_labels)\n",
    "    idx_train = torch.LongTensor(sample_data.train_id)\n",
    "    idx_val = torch.LongTensor(sample_data.valid_id)\n",
    "    idx_test = torch.LongTensor(sample_data.test_id)\n",
    "    for epoch in range(0, epochs):\n",
    "        model_mGCN.train()\n",
    "        optimizer.zero_grad()\n",
    "        # print(training_negative.shape)\n",
    "        logits = model_mGCN(sample_data.dataset)\n",
    "        # labels = split_edges[i]['train']['label']\n",
    "        loss = criterion(logits[sample_data.train_id], sample_data.labels[sample_data.train_id])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        \n",
    "        pred = logits.max(1).indices\n",
    "        \n",
    "        macro,micro = evaluate_model(sample_data.test_id)\n",
    "        micro_list.append(micro)\n",
    "        macro_list.append(macro)\n",
    "        \n",
    "        #sim = run_similarity_search(test_embs, test_lbls)\n",
    "        #sim = sim[0]\n",
    "        #sim_list.append(sim)\n",
    "        #k1_list.append(k1)\n",
    "        # write_results(test_id)\n",
    "        if macro > best_macro and micro> best_micro:\n",
    "            best_macro = macro\n",
    "            best_micro = micro\n",
    "            print('Epoch:', epoch)\n",
    "            print(\"Macro_F1:\", macro)\n",
    "            print(\"Micro_F1:\", micro)\n",
    "            #print(\"NMI:\", nmi)\n",
    "            \n",
    "    features = torch.FloatTensor(preprocessed_features)\n",
    "    gcn_adj_list = [normalize_adj(adj) for adj in sample_data.gcn_adj_list]\n",
    "    adj_list = [sparse_mx_to_torch_sparse_tensor(adj) for adj in gcn_adj_list]\n",
    "    embeds = embed(features, adj_list, sparse)        \n",
    "    print('Final result: ')\n",
    "    print(\"Macro_F1:\", best_macro)\n",
    "    print(\"Micro_F1:\", best_micro)        \n",
    "    evaluate(embeds, idx_train, idx_val, idx_test, labels)\n",
    "    \n",
    "\n",
    "\n",
    "\n",
    "results = []\n",
    "# results_hits = {}\n",
    "# for K in [20, 50, 100]:\n",
    "#     results_hits[f'Hits@{K}'] = []\n",
    "for run in range(0, args.runs):\n",
    "    np.random.seed(run)\n",
    "    torch.manual_seed(run)\n",
    "    torch.cuda.manual_seed(run)\n",
    "    random.seed(run)\n",
    "\n",
    "\n",
    "sample_data = dataset(args.dataset)\n",
    "preprocessed_features = preprocess_features(sample_data.features)\n",
    "ft_size = preprocessed_features[0].shape[1] \n",
    "hid_units = 128\n",
    "n_networks = len(sample_data.adj_list)\n",
    "taskname  = 'node'\n",
    "\n",
    "#training_id, valid_id, test_id = split_node_data(len(sample_data.labels),train_percent=args.training_ratio,valid_percent = 0.1)\n",
    "#data, num_views, training_id, valid_id, test_id, num_classes, labels, adj_list, edge_list = load_data(args.dataset, training_percent=args.training_ratio)\n",
    "# data, split_edges = split_data(data_ori, args.test_view, multi=True)\n",
    "print(\"Finish loading data\")\n",
    "num_feat = sample_data.dataset.x.shape[1]\n",
    "model_mGCN = mGCN(num_feat, args.hidden, None, sample_data.num_dims, args.alpha, sample_data.num_classes, dropout=args.dropout)\n",
    "criterion = nn.NLLLoss()\n",
    "optimizer = optim.Adam(model_mGCN.parameters(), lr=args.lr)\n",
    "train_model(args.epochs)\n",
    "evaluate_model(sample_data.valid_id)  \n",
    "\n",
    "#for epoch in range(args.epochs):\n",
    "    #train()\n",
    "    \n",
    "print(\"Model training is complete\")\n",
    "\n",
    "#test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc4faf26",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
